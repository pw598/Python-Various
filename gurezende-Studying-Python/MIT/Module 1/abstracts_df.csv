,text
0,"        We present the observational results from a detailed timing analysis of the black hole candidate EXO 1846-031 during its outburst in 2019 with the observations of Insight-HXMT, NICER and MAXI. This outburst can be classfied roughly into four different states. Type-C quasi-periodic oscillations (QPOs) observed by NICER (about 0.1-6Hz) and Insight-HXMT (about 0.7-8Hz) are also reported in this work. Meanwhile, we study various physical quantities related to QPO frequency.The QPO rms-frequency relationship in three energy band 1-10 keV indicates that there is a turning pointing in frequency around 2 Hz,which is similar to that of GRS 1915+105. A possible hypothesis for the relationship above may be related to the inclination of the source, which may require a high inclination to explain it. The relationships between QPO frequency and QPO rms,hardness,total fractional rms and count rate have also been found in other transient sources, which can indicate that the origin of type-C QPOs is non-thermal.        "
1,"        Low-frequency quasi-periodic oscillations (LFQPOs) are commonly found in black hole X-ray binaries, and their origin is still under debate. The properties of LFQPOs at high energies (above 30 keV) are closely related to the nature of the accretion flow in the innermost regions, and thus play a crucial role in critically testing various theoretical models. The Hard X-ray Modulation Telescope (Insight-HXMT) is capable of detecting emissions above 30 keV, and is therefore an ideal instrument to do so. Here we report the discovery of LFQPOs above 200 keV in the new black hole MAXI J1820+070 in the X-ray hard state, which allows us to understand the behaviours of LFQPOs at hundreds of kiloelectronvolts. The phase lag of the LFQPO is constant around zero below 30 keV, and becomes a soft lag (that is, the high-energy photons arrive first) above 30 keV. The soft lag gradually increases with energy and reaches ~0.9s in the 150-200 keV band. The detection at energies above 200 keV, the large soft lag and the energy-related behaviors of the LFQPO pose a great challenge for most currently existing models, but suggest that the LFQPO probably originates from the precession of a small-scale jet.        "
2,"        Machine learning has demonstrated great power in materials design, discovery, and property prediction. However, despite the success of machine learning in predicting discrete properties, challenges remain for continuous property prediction. The challenge is aggravated in crystalline solids due to crystallographic symmetry considerations and data scarcity. Here we demonstrate the direct prediction of phonon density of states using only atomic species and positions as input. We apply Euclidean neural networks, which by construction are equivariant to 3D rotations, translations, and inversion and thereby capture full crystal symmetry, and achieve high-quality prediction using a small training set of $\sim 10^{3}$ examples with over 64 atom types. Our predictive model reproduces key features of experimental data and even generalizes to materials with unseen elements. We demonstrate the potential of our network by predicting a broad number of high phononic specific heat capacity materials. Our work indicates an efficient approach to explore materials' phonon structure, and can further enable rapid screening for high-performance thermal storage materials and phonon-mediated superconductors.        "
3,"        In this paper, we study $(G,Ï_Ï)$-equivariant $Ï$-coordinated quasi modules for nonlocal vertex algebras. Among the main results, we establish several conceptual results, including a generalized commutator formula and a general construction of weak quantum vertex algebras and their $(G,Ï_Ï)$-equivariant $Ï$-coordinated quasi modules. As an application, we also construct (equivariant) $Ï$-coordinated quasi modules for lattice vertex algebras by using Lepowsky's work on twisted vertex operators.        "
4,"        A polarization-independent reconfigurable frequency selective rasorber (FSR)/absorber with low insertion loss based on diodes is proposed in this paper. The presented structure consists of a lossy layer based on square loops and a bandpass frequency-selective surface. These two layers are separated by an air layer. Each layer has an embedded bias network that provides the bias voltage to the diodes through metallic via. This configuration can avoid undesirable effects associated with the additional biasing wire. When the diodes are in off-state, the structure is in FSR mode and exhibits a transmission window at 4.28GHz with only 0.69dB insertion loss (IL) within the absorption bands. While diodes are in on-state and the structure switches to absorber mode, it achieves perfect absorption with absorptivity of over 90% ranging from 2.8 to 5.2 GHz. An equivalent circuit model (ECM) is developed to analyse the physical mechanism of the structure. A prototype of the proposed architecture is fabricated and measured, where reasonable agreements between simulations and measurements are observed, verifying the effectiveness of this design.        "
5,"        Deep learning is a hot research topic in the field of machine learning methods and applications. Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but both of them are difficult to train since they need to train the generator (or encoder) and the discriminator (or decoder) simultaneously, which is easy to cause unstable training. In order to solve or alleviate the synchronous training difficult problems of GANs and VAEs, recently, researchers propose Generative Scattering Networks (GSNs), which use wavelet scattering networks (ScatNets) as the encoder to obtain the features (or ScatNet embeddings) and convolutional neural networks (CNNs) as the decoder to generate the image. The advantage of GSNs is the parameters of ScatNets are not needed to learn, and the disadvantage of GSNs is that the expression ability of ScatNets is slightly weaker than CNNs and the dimensional reduction method of Principal Component Analysis (PCA) is easy to lead overfitting in the training of GSNs, and therefore affect the generated quality in the testing process. In order to further improve the quality of generated images while keep the advantages of GSNs, this paper proposes Generative Fractional Scattering Networks (GFRSNs), which use more expressive fractional wavelet scattering networks (FrScatNets) instead of ScatNets as the encoder to obtain the features (or FrScatNet embeddings) and use the similar CNNs of GSNs as the decoder to generate the image. Additionally, this paper develops a new dimensional reduction method named Feature-Map Fusion (FMF) instead of PCA for better keeping the information of FrScatNets and the effect of image fusion on the quality of image generation is also discussed.        "
6,"        Reconfigurable photonic systems featuring minimal power consumption are crucial for integrated optical devices in real-world technology. Current active devices available in foundries, however, use volatile methods to modulate light, requiring a constant supply of power and significant form factors. Essential aspects to overcoming these issues are the development of nonvolatile optical reconfiguration techniques which are compatible with on-chip integration with different photonic platforms and do not disrupt their optical performances. In this paper, a solution is demonstrated using an optoelectronic framework for nonvolatile tunable photonics that employs undoped-graphene microheaters to thermally and reversibly switch the optical phase-change material Ge$_2$Sb$_2$Se$_4$Te$_1$ (GSST). An in-situ Raman spectroscopy method is utilized to demonstrate, in real-time, reversible switching between four different levels of crystallinity. Moreover, a 3D computational model is developed to precisely interpret the switching characteristics, and to quantify the impact of current saturation on power dissipation, thermal diffusion, and switching speed. This model is used to inform the design of nonvolatile active photonic devices; namely, broadband Si$_3$N$_4$ integrated photonic circuits with small form-factor modulators and reconfigurable metasurfaces displaying 2$Ï$ phase coverage through neural-network-designed GSST meta-atoms. This framework will enable scalable, low-loss nonvolatile applications across a diverse range of photonics platforms.        "
7,"        In this paper, for an arbitrary Kac-Moody Lie algebra $\mathfrak{g}$ and a diagram automorphism $Î¼$ of $\mathfrak{g}$ satisfying two linking conditions, we introduce and study a $Î¼$-twisted quantum affinization algebra $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$ of $\mathfrak{g}$. When $\mathfrak{g}$ is of finite type, $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$ is Drinfeld's current algebra realization of the twisted quantum affine algebra. And, when $Î¼=\mathrm{Id}$, $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$ is the quantum affinization algebra introduced by Ginzburg-Kapranov-Vasserot. As the main results of this paper, we first prove a triangular decomposition of $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$. Second, we give a simple characterization of the affine quantum Serre relations on restricted $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$-modules in terms of ""normal order products"". Third, we prove that the category of restricted $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$-modules is a monoidal category and hence obtain a topological Hopf algebra structure on the ""restricted completion"" of $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$. Fourth, we study the classical limit of $\mathcal{U}_{\hbar}(\hat{\mathfrak{g}}_Î¼)$ and abridge it to the quantization theory of extended affine Lie algebras. In particular, based on a classification result of Allison-Berman-Pianzola, we obtain the $\hbar$-deformation of nullity $2$ extended affine Lie algebras.        "
8,"        In this paper we introduce a new quantum algebra which specializes to the $2$-toroidal Lie algebra of type $A_1$.  We prove that this quantum toroidal algebra has a natural triangular decomposition, a (topological) Hopf algebra structure and a vertex operator realization.        "
9,"        Fluorescein angiography can provide a map of retinal vascular structure and function, which is commonly used in ophthalmology diagnosis, however, this imaging modality may pose risks of harm to the patients. To help physicians reduce the potential risks of diagnosis, an image translation method is adopted. In this work, we proposed a conditional generative adversarial network(GAN) - based method to directly learn the mapping relationship between structure fundus images and fundus fluorescence angiography images. Moreover, local saliency maps, which define each pixel's importance, are used to define a novel saliency loss in the GAN cost function. This facilitates more accurate learning of small-vessel and fluorescein leakage features.        "
10,"        The interaction between off-resonant laser pulses and excitons in monolayer transition metal dichalcogenides is attracting increasing interest as a route for the valley-selective coherent control of the exciton properties. Here, we extend the classification of the known off-resonant phenomena by unveiling the impact of a strong THz field on the excitonic resonances of monolayer MoS$_2$. We observe that the THz pump pulse causes a selective modification of the coherence lifetime of the excitons, while keeping their oscillator strength and peak energy unchanged. We rationalize these results theoretically by invoking a hitherto unobserved manifestation of the Franz-Keldysh effect on an exciton resonance. As the modulation depth of the optical absorption reaches values as large as 0.05 dBm at room temperature, our findings open the way to the use of semiconducting transition metal dichalcogenides as compact and efficient platforms for high-speed electroabsorption devices.        "
11,"        This paper reviews the NTIRE 2020 Challenge on NonHomogeneous Dehazing of images (restoration of rich details in hazy image). We focus on the proposed solutions and their results evaluated on NH-Haze, a novel dataset consisting of 55 pairs of real haze free and nonhomogeneous hazy images recorded outdoor. NH-Haze is the first realistic nonhomogeneous haze dataset that provides ground truth images. The nonhomogeneous haze has been produced using a professional haze generator that imitates the real conditions of haze scenes. 168 participants registered in the challenge and 27 teams competed in the final testing phase. The proposed solutions gauge the state-of-the-art in image dehazing.        "
12,"        Angle closure glaucoma (ACG) is a more aggressive disease than open-angle glaucoma, where the abnormal anatomical structures of the anterior chamber angle (ACA) may cause an elevated intraocular pressure and gradually lead to glaucomatous optic neuropathy and eventually to visual impairment and blindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging provides a fast and contactless way to discriminate angle closure from open angle. Although many medical image analysis algorithms have been developed for glaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In particular, there is no public AS-OCT dataset available for evaluating the existing methods in a uniform way, which limits progress in the development of automated techniques for angle closure detection and assessment. To address this, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held in conjunction with MICCAI 2019. The AGE challenge consisted of two tasks: scleral spur localization and angle closure classification. For this challenge, we released a large dataset of 4800 annotated AS-OCT images from 199 patients, and also proposed an evaluation framework to benchmark and compare different models. During the AGE challenge, over 200 teams registered online, and more than 1100 results were submitted for online evaluation. Finally, eight teams participated in the onsite challenge. In this paper, we summarize these eight onsite challenge methods and analyze their corresponding results for the two tasks. We further discuss limitations and future directions. In the AGE challenge, the top-performing approach had an average Euclidean Distance of 10 pixels (10um) in scleral spur localization, while in the task of angle closure classification, all the algorithms achieved satisfactory performances, with two best obtaining an accuracy rate of 100%.        "
13,"        In this paper, a low radar cross section (RCS) patch antenna based on the 3-bit metasurface composed of linear polarization conversion elements is designed. At first, 3-bit coding metamaterials are constructed by a sequence of eight coded unit cells, which have a similar cross-polarized reflected amplitude response and gradient reflected phase responses covering 0~2Ï, respectively. Equivalent circuit models (ECMs) of these unit cells are created to describe their electrical behavior for the two linear incident polarizations at the same time. Then, a patch antenna is integrated on the 3-bit metasurface, of which the elements are placed with a 2-dimensional linear coding sequence. The metal square ring is set around the patch antenna to protect it from the disturbance of metasurface. Both the simulation and experiment results demonstrate that the designed metasurface can primarily reduce the antenna RCS at a broadband, while the antenna performances are not degraded significantly.        "
14,"        An intelligent radome utilizing composite metamaterial structures is presented and investigated in this article, which can realize energy isolation and asymmetric propagation of electromagnetic (EM) wave self-adaptively by controlling states of PIN diodes. The whole structure mainly consists of a broadband polarization-sensitive polarization converter (PC) and an active frequency selective rasorber (AFSR) switching between a transmission mode and absorption mode which is used as an energy-selective surface (ESS). Among them, the function of the PC is to make the EM waves transmit asymmetrically, and the purpose of AFSR is to make the high-power waves be reflected or absorbed, which depends on the polarization type of the wave. Thus, the radome can realize both asymmetric propagations of EM wave and electromagnetic shielding. The equivalent circuit models (ECM) and parametric studies are considered to explain the physical operating mechanism of PC and AFSR. The fabricated structure with 7*7 unit cells is experimentally demonstrated and the measured results agree with simulated results well. Considering the distinctive characteristic of self-actuation, the presented concept has the potential application in electromagnetic stealth and HPEMWs shielding to protect communication devices.        "
15,"        Automatic analysis of highly crowded people has attracted extensive attention from computer vision research. Previous approaches for crowd counting have already achieved promising performance across various benchmarks. However, to deal with the real situation, we hope the model run as fast as possible while keeping accuracy. In this paper, we propose a compact convolutional neural network for crowd counting which learns a more efficient model with a small number of parameters. With three parallel filters executing the convolutional operation on the input image simultaneously at the front of the network, our model could achieve nearly real-time speed and save more computing resources. Experiments on two benchmarks show that our proposed method not only takes a balance between performance and efficiency which is more suitable for actual scenes but also is superior to existing light-weight models in speed.        "
16,"        Active Object Tracking (AOT) is crucial to many visionbased applications, e.g., mobile robot, intelligent surveillance. However, there are a number of challenges when deploying active tracking in complex scenarios, e.g., target is frequently occluded by obstacles. In this paper, we extend the single-camera AOT to a multi-camera setting, where cameras tracking a target in a collaborative fashion. To achieve effective collaboration among cameras, we propose a novel Pose-Assisted Multi-Camera Collaboration System, which enables a camera to cooperate with the others by sharing camera poses for active object tracking. In the system, each camera is equipped with two controllers and a switcher: The vision-based controller tracks targets based on observed images. The pose-based controller moves the camera in accordance to the poses of the other cameras. At each step, the switcher decides which action to take from the two controllers according to the visibility of the target. The experimental results demonstrate that our system outperforms all the baselines and is capable of generalizing to unseen environments. The code and demo videos are available on our website https://sites.google.com/view/pose-assistedcollaboration.        "
17,"        We study the relation between the properties of the bulge/disc components and the HI mass fraction of galaxies. We find that at fixed stellar mass, disc colours are correlated with the HI mass fraction, while bulge colours are not. The lack of a correlation between the bulge colour and the HI mass fraction is regardless whether the bulges are pseudo, or whether the galaxies host bars or are interacting with a neighbour. There is no strong correlation between the colours of the discs and bulges either. These results suggest that the current total amount of HI is closely related to the formation of discs, but does not necessarily fuel the formation of (pseudo) bulges in an efficient way. We do not find evidence for the star formation in the discs to be quenched by the bulges.        "
18,"        This paper documents the sixteenth data release (DR16) from the Sloan Digital Sky Surveys; the fourth and penultimate from the fourth phase (SDSS-IV). This is the first release of data from the southern hemisphere survey of the Apache Point Observatory Galactic Evolution Experiment 2 (APOGEE-2); new data from APOGEE-2 North are also included. DR16 is also notable as the final data release for the main cosmological program of the Extended Baryon Oscillation Spectroscopic Survey (eBOSS), and all raw and reduced spectra from that project are released here. DR16 also includes all the data from the Time Domain Spectroscopic Survey (TDSS) and new data from the SPectroscopic IDentification of ERosita Survey (SPIDERS) programs, both of which were co-observed on eBOSS plates. DR16 has no new data from the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey (or the MaNGA Stellar Library ""MaStar""). We also preview future SDSS-V operations (due to start in 2020), and summarize plans for the final SDSS-IV data release (DR17).        "
19,"        We investigate the effect of lattice disorder and local correlation effects in finite and periodic silicene structures caused by carbon doping using first-principles calculations. For both finite and periodic silicene structures, the electronic properties carbon-doped monolayers are dramatically changed by controlling the doping sites in the structures, which is related to the amount of disorder introduced in the lattice and electron-electron correlation effects. By changing the position of the carbon dopants, we found that a Mott-Anderson transition is achieved. Moreover, the band gap is determined by the level of lattice disorder and electronic correlation effects. Finally, these structures are ferromagnetic even under disorder which has potential applications in Si-based nanoelectronics, such as field-effect transistors (FETs).        "
20,"        The accuracy of OCR is usually affected by the quality of the input document image and different kinds of marred document images hamper the OCR results. Among these scenarios, the low-resolution image is a common and challenging case. In this paper, we propose the cascaded networks for document image super-resolution. Our model is composed by the Detail-Preserving Networks with small magnification. The loss function with perceptual terms is designed to simultaneously preserve the original patterns and enhance the edge of the characters. These networks are trained with the same architecture and different parameters and then assembled into a pipeline model with a larger magnification. The low-resolution images can upscale gradually by passing through each Detail-Preserving Network until the final high-resolution images. Through extensive experiments on two scanning document image datasets, we demonstrate that the proposed approach outperforms recent state-of-the-art image super-resolution methods, and combining it with standard OCR system lead to signification improvements on the recognition results.        "
21,"        The stacking and bending of graphene are trivial but extremely powerful agents of control over graphene's manifold physics. By changing the twist angle, one can drive the system over a plethora of exotic states via strong electron correlation, thanks to the moirÃ© superlattice potentials, while the periodic or triaxial strains induce discretization of the band structure into Landau levels without the need for an external magnetic field. We fabricated a hybrid system comprising both the stacking and bending tuning knobs. We have grown the graphene monolayers by chemical vapor deposition, using $^{12}$C and $^{13}$C precursors, which enabled us to individually address the layers through Raman spectroscopy mapping. We achieved the long-range spatial modulation by sculpturing the top layer ($^{13}$C) over uniform magnetic nanoparticles (NPs) deposited on the bottom layer ($^{12}$C). An atomic force microscopy study revealed that the top layer tends to relax into pyramidal corrugations with C$_3$ axial symmetry at the position of the NPs, which have been widely reported as a source of large pseudomagnetic fields (PMFs) in graphene monolayers. The modulated graphene bilayer (MGBL) also contains a few micrometer large domains, with the twist angle ~ 10$^{\circ}$, which were identified via extreme enhancement of the Raman intensity of the G-mode due to formation of Van Hove singularities (VHSs). We thereby conclude that the twist induced VHSs coexist with the PMFs generated in the strained pyramidal objects without mutual disturbance. The graphene bilayer modulated with magnetic NPs is a non-trivial hybrid system that accommodates features of twist induced VHSs and PMFs in environs of giant classical spins.        "
22,"        As China's first X-ray astronomical satellite, the Hard X-ray Modulation Telescope (HXMT), which was dubbed as Insight-HXMT after the launch on June 15, 2017, is a wide-band (1-250 keV) slat-collimator-based X-ray astronomy satellite with the capability of all-sky monitoring in 0.2-3 MeV. It was designed to perform pointing, scanning and gamma-ray burst (GRB) observations and, based on the Direct Demodulation Method (DDM), the image of the scanned sky region can be reconstructed. Here we give an overview of the mission and its progresses, including payload, core sciences, ground calibration/facility, ground segment, data archive, software, in-orbit performance, calibration, background model, observations and some preliminary results.        "
23,"        The Hard X-ray Modulation Telescope (HXMT) named Insight is China's first X-ray astronomical satellite. The Low Energy X-ray Telescope (LE) is one of its main payloads onboard. The detectors of LE adopt swept charge device CCD236 with L-shaped transfer electrodes. Charges in detection area are read out continuously along specific paths, which leads to a time response distribution of photons readout time. We designed a long exposure readout mode to measure the time response distribution. In this mode, CCD236 firstly performs exposure without readout, then all charges generated in preceding exposure phase are read out completely. Through analysis of the photons readout time in this mode, we obtained the probability distribution of photons readout time.        "
24,"        We combine the published stellar mass function (SMF) and gas scaling relations to explore the baryonic (stellar plus cold gas) mass function (BMF) of galaxies to redshift $z=3$. We find evidence that at log$(M_{\rm baryon}/M_{\bigodot})&gt;11.3$, the BMF evolves little since $z\sim 2.2$. With the evolution of BMF and SMF, we investigate the baryon net accretion rate ($\dotÏ_{\rm baryon}$) and stellar mass growth rate ($\dotÏ_{\rm star}$) for the galaxy population of log($M_{\rm star}/M_{\bigodot}$)&gt;10. The ratio between these two quanties, $\dotÏ_{\rm baryon}$/$\dotÏ_{\rm star}$, decreases from $\dotÏ_{\rm baryon}$/$\dotÏ_{\rm star}\sim$2 at $z\sim 2.5$ to $\dotÏ_{\rm baryon}$/$\dotÏ_{\rm star}&lt;$0.5 at $z\sim 0.5$, suggesting that massive galaxies are transforming from the ""accretion dominated"" phase to the ""depletion dominated"" phase from high$-z$ to low$-z$. The transition of these two phases occurs at $z\sim1.5$, which is consistent with the onset redshift of the decline of cosmic star formation rate density. This provides evidence to support the idea that the decline of cosmic star formation rate density since $z\sim1.5$ is mainly resulted from the decline of baryon net accretion rate and star formation quenching in galaxies.        "
25,"        We unveil the diamondization mechanism of few-layer graphene compressed in the presence of water, providing robust evidence for the pressure-induced formation of 2D diamond. High-pressure Raman spectroscopy provides evidence of a phase transition occurring in the range of 4-7 GPa for 5-layer graphene and graphite. The pressure-induced phase is partially transparent and indents the silicon substrate. Our combined theoretical and experimental results indicate a gradual top-bottom diamondization mechanism, consistent with the formation of diamondene, a 2D ferromagnetic semiconductor. High-pressure x-ray diffraction on graphene indicates the formation of hexagonal diamond, consistent with the bulk limit of eclipsed-conformed diamondene.        "
26,"        Environment is one of the key external drivers of the galaxies, while active galactic nucleus (AGN) is one of the key internal drivers. Both of them play fundamental roles in regulating the formation and evolution of galaxies. We explore the interrelationship between environment and AGN in SDSS. At a given stellar mass, the specific star formation rate distribution of the AGN host galaxies remains unchanged with over-density, with the peak of the distribution around the Green Valley. We show that, at a given stellar mass, the AGN fraction that has been commonly used in previous studies (defined as the number of AGNs relative to all galaxies including passive and star forming ones) does decrease with increasing over-density for satellites. This is largely due to the fact that the fraction of passive galaxies strongly depends on environment. In order to investigate the intrinsic correlation between AGN and environment, especially under the assumption that AGN feedback is responsible for star formation quenching, the AGN fraction should be defined as the number of AGNs relative to the star-forming galaxies only. With the new definition, we find little dependence of AGN fraction on over-density, central/satellite, and group halo mass. There is only marginal evidence that AGN may prefer denser regions, which is possibly due to more frequent interaction of galaxies or higher merger rate in groups. Our results support the scenario that internal secular evolution is the predominant mechanism of triggering AGN activity, while external environment related processes only play a minor role.        "
27,"        We explore the interrelationships between the galaxy group halo mass and various observable group properties. We propose a simple scenario that describes the evolution of the central galaxies and their host dark matter halos. Star formation quenching is one key process in this scenario, which leads to the different assembly histories of blue groups (group with a blue central) and red groups (group with a red central). For blue groups, both the central galaxy and the halo continue to grow their mass. For red groups, the central galaxy has been quenched and its stellar mass remains about constant, while its halo continues to grow by merging smaller halos. From this simple scenario, we speculate about the driving properties that should strongly correlate with the group halo mass. We then apply the machine learning algorithm the Random Forest (RF) regressor to blue groups and red groups separately in the semianalytical model L-GALAXIES to explore these nonlinear multicorrelations and to verify the scenario as proposed above. Remarkably, the results given by the RF regressor are fully consistent with the prediction from our simple scenario and hence provide strong support for it. As a consequence, the group halo mass can be more accurately determined from observable galaxy properties by the RF regressor with a 50% reduction in error. A halo mass more accurately determined in this way also enables more accurate investigations on the galaxy-halo connection and other important related issues, including galactic conformity and the effect of halo assembly bias on galaxy assembly.        "
28,"        Advanced microscopy and/or spectroscopy tools play indispensable role in nanoscience and nanotechnology research, as it provides rich information about the growth mechanism, chemical compositions, crystallography, and other important physical and chemical properties. However, the interpretation of imaging data heavily relies on the ""intuition"" of experienced researchers. As a result, many of the deep graphical features obtained through these tools are often unused because of difficulties in processing the data and finding the correlations. Such challenges can be well addressed by deep learning. In this work, we use the optical characterization of two-dimensional (2D) materials as a case study, and demonstrate a neural-network-based algorithm for the material and thickness identification of exfoliated 2D materials with high prediction accuracy and real-time processing capability. Further analysis shows that the trained network can extract deep graphical features such as contrast, color, edges, shapes, segment sizes and their distributions, based on which we develop an ensemble approach topredict the most relevant physical properties of 2D materials. Finally, a transfer learning technique is applied to adapt the pretrained network to other applications such as identifying layer numbers of a new 2D material, or materials produced by a different synthetic approach. Our artificial-intelligence-based material characterization approach is a powerful tool that would speed up the preparation, initial characterization of 2D materials and other nanomaterials and potentially accelerate new material discoveries.        "
29,"        2D van der Waals materials have rich and unique functional properties, but many are susceptible to corrosion under ambient conditions. Here we show that linear alkylamines are highly effective in protecting the optoelectronic properties of these materials such as black phosphorous (BP) and transitional metal dichalcogenides. As a representative example, n-hexylamine can be applied in the form of thin molecular monolayers on BP flakes with less-than-2nm thickness and can prolong BP's lifetime from a few hours to several weeks and even months in ambient environments. Characterizations combined with our theoretical analysis show that the thin monolayers selectively sift out water molecules, forming a drying layer to achieve the passivation of the protected 2D materials. The monolayer coating is also stable in air, hydrogen annealing, and organic solvents, but can be removed by certain organic acids.        "
30,"        How Supermassive Blackholes (SMBHs) are spun-up is a key issue of modern astrophysics. As an extension of the study in Wang et al. (2016), we here address the issue by comparing the host galaxy properties of nearby ($z&lt;0.05$) radio-selected Seyfert 2 galaxies. With the two-dimensional bulge+disk decompositions for the SDSS $r$-band images, we identify a dichotomy on various host galaxy properties for the radio-powerful SMBHs. By assuming the radio emission from the jet reflects a high SMBH spin, which stems from the well-known BZ mechanism of jet production, high-mass SMBHs (i.e., $M_{\mathrm{BH}}&gt;10^{7.9}M_\odot$) have a preference for being spun-up in classical bulges, and low-mass SMBHs (i.e., $M_{\mathrm{BH}}=10^{6-7}M_\odot$) in pseudo-bulges. This dichotomy suggests and confirms that high-mass and low-mass SMBHs are spun-up in different ways, i.e., a major ""dry"" merger and a secular evolution.        "
31,"        The I4U consortium was established to facilitate a joint entry to NIST speaker recognition evaluations (SRE). The latest edition of such joint submission was in SRE 2018, in which the I4U submission was among the best-performing systems. SRE'18 also marks the 10-year anniversary of I4U consortium into NIST SRE series of evaluation. The primary objective of the current paper is to summarize the results and lessons learned based on the twelve sub-systems and their fusion submitted to SRE'18. It is also our intention to present a shared view on the advancements, progresses, and major paradigm shifts that we have witnessed as an SRE participant in the past decade from SRE'08 to SRE'18. In this regard, we have seen, among others, a paradigm shift from supervector representation to deep speaker embedding, and a switch of research challenge from channel compensation to domain adaptation.        "
32,"        We study the gas inflow rate ($Î¶_{\rm inflow}$) and outflow rate ($Î¶_{\rm outflow}$) evolution of local Milky Way-mass star-forming galaxies (SFGs) since $z=1.3$. The stellar mass growth history of Milky Way-mass progenitor SFGs is inferred from the evolution of the star formation rate (SFR)$-$stellar mass ($M_{\ast}$) relation, and the gas mass ($M_{\rm gas}$) is derived using the recently established gas scaling relations. With the $M_{\ast}+M_{\rm gas}$ growth curve, the net inflow rate $Îº$ is quantified at each cosmic epoch. At $z\sim 1.3$, $Îº$ is comparable with the SFR, whereas it rapidly decreases to $\sim 0.15\times$SFR at $z=0$. We then constrain the average outflow rate $Î¶_{\rm outflow}$ of progenitor galaxies by modeling the evolution of their gas-phase metallicity. The best-fit $Î¶_{\rm outflow}$ is found to be $(0.5-0.8)\times$SFR. Combining $Îº$ and $Î¶_{\rm outflow}$, we finally investigate the evolution of $Î¶_{\rm inflow}$ since $z=1.3$. We find that $Î¶_{\rm inflow}$ rapidly decreases by $\sim$80\% from $z=1.3$ to $z=0.5$. At $z&lt;0.5$, $Î¶_{\rm inflow}$ continuously decreases but with a much lower decreasing rate. Implications of these findings on galaxy evolution are discussed.        "
33,"        Let $\mathfrak{g}$ be the derived subalgebra of a Kac-Moody Lie algebra of finite type or affine type, $Î¼$ a diagram automorphism of $\mathfrak{g}$ and $L(\mathfrak{g},Î¼)$ the loop algebra of $\mathfrak{g}$ associated to $Î¼$. In this paper, by using the vertex algebra technique, we provide a general construction of current type presentations for the universal central extension $\widehat{\mathfrak{g}}[Î¼]$ of $L(\mathfrak{g},Î¼)$. The construction contains the classical limit of Drinfeld's new realization for (twisted and untwisted) quantum affine algebras ([Dr]) and the Moody-Rao-Yokonuma presentation for toroidal Lie algebras ([MRY]) as special examples. As an application, when $\mathfrak{g}$ is of simply-laced type, we prove that the classical limit of the $Î¼$-twisted quantum affinization of the quantum Kac-Moody algebra associated to $\mathfrak{g}$ introduced in [CJKT1] is the universal enveloping algebra of $\widehat{\mathfrak{g}}[Î¼]$.        "
34,"        Let $\fg$ be an affine Kac-Moody algebra, and $Î¼$ a diagram automorphism of $\fg$. In this paper, we give an explicit realization for the universal central extension $\wh\fg[Î¼]$ of the twisted loop algebra of $\fg$ related to $Î¼$, which provides a Moody-Rao-Yokonuma presentation for the algebra $\wh\fg[Î¼]$ when $Î¼$ is non-transitive, and the presentation is indeed related to the quantization of toroidal Lie algebras.        "
35,"        When the Fermi level matches the Dirac point in graphene, the reduced charge screening can dramatically enhance electron-electron (e-e) scattering to produce a strongly interacting Dirac liquid. While the dominance of e-e scattering already leads to novel behaviors, such as electron hydrodynamic flow, further exotic phenomena have been predicted to arise specifically from the unique kinematics of e-e scattering in massless Dirac systems. Here, we use optoelectronic probes, which are highly sensitive to the kinematics of electron scattering, to uncover a giant intrinsic photocurrent response in pristine graphene. This photocurrent emerges exclusively at the charge neutrality point and vanishes abruptly at non-zero charge densities. Moreover, it is observed at places with broken reflection symmetry, and it is selectively enhanced at free graphene edges with sharp bends. Our findings reveal that the photocurrent relaxation is strongly suppressed by a drastic change of fast photocarrier kinematics in graphene when its Fermi level matches the Dirac point. The emergence of robust photocurrents in neutral Dirac materials promises new energy-harvesting functionalities and highlights intriguing electron dynamics in the optoelectronic response of Dirac fluids.        "
36,"        Two-dimensional (2D) materials are promising candidates for next-generation electronic devices. In this regime, insulating 2D ferromagnets, which remain rare, are of special importance due to their potential for enabling new device architectures. Here we report the discovery of ferromagnetism in a layered van der Waals semiconductor, VI3, which is based on honeycomb vanadium layers separated by an iodine-iodine van der Waals gap. It has a BiI3-type structure (R-3, No.148) at room temperature, and our experimental evidence suggests that it may undergo a subtle structural phase transition at 78 K. VI3 becomes ferromagnetic at 49 K, below which magneto-optical Kerr effect imaging clearly shows ferromagnetic domains, which can be manipulated by the applied external magnetic field. The optical band gap determined by reflectance measurements is 0.6 eV, and the material is highly resistive.        "
37,"        Hexagonal boron nitride (h-BN) is a two dimensional (2D) layered insulator with superior dielectric performance that offers excellent interaction with other 2D materials (e.g. graphene, MoS2). Large-area h-BN can be readily grown on metallic substrates via chemical vapor deposition (CVD), but the impact of local inhomogeneities on the electrical properties of the h-BN and their effect in electronic devices is unknown. Here it is shown that the electrical properties of h-BN stacks grown on polycrystalline Pt vary a lot depending on the crystalline orientation of the Pt grain, but within the same grain the electrical properties of the h-BN are very homogeneous. The reason is that the thickness of the CVD-grown h-BN stack is different on each Pt grain. Conductive atomic force microscopy (CAFM) maps show that the tunneling current across the h-BN stack fluctuates up to 3 orders of magnitude from one Pt grain to another. However, probe station experiments reveal that the variability of electronic devices fabricated within the same Pt grain is surprisingly small. As cutting-edge electronic devices are ultra-scaled, and as the size of the metallic substrate grains can easily exceed 100 Î¼m (in diameter), CVD-grown h-BN stacks may be useful to fabricate electronic devices with low variability.        "
38,"        Through a combination of rheological characterization and temperature-variable imaging methods, a novel gelation pathway in dilute solutions of a semiconducting polymer to achieve interconnected, crystalline networks with hierarchical porosity is reported. Upon rapid cooling, solutions of regioregular poly(3-hexylthiophene) (RR-P3HT) in ortho-dichlorobenzene formed thermoreversible gels. Temperature-variable confocal microscopy revealed cooling-induced structural rearrangement to progress through viscoelastic phase separation. The phase separation process arrested prematurely during the formation of micron-sized solvent-rich ""holes"" within the RR-P3HT matrix due to intrachain crystallization. Cryogen-based scanning electron microscopy of RR P3HT gels revealed the existence of an interfibrillar network exhibiting nano-sized pores. Remarkably, these networks formed to equal gel strengths when a third component, either small molecule phenyl C61 butyric acid methyl ester (PCBM) or non-crystallizing regiorandom (Rra)-P3HT, was added to the solution. Organic solar cells in which the active layers were deposited from phase-separated solutions displayed 45% higher efficiency compared to reference cells.        "
39,        In this paper we generalize Drinfeld's twisted quantum affine algebras to construct twisted quantum algebras for all simply-laced generalized Cartan matrices and present their vertex representation realizations.        
40,"        Property by design is one appealing idea in material synthesis but hard to achieve in practice. A recent successful example is the demonstration of van der Waals (vdW) heterostructures,1-3 in which atomic layers are stacked on each other and different ingredients can be combined beyond symmetry and lattice matching. This concept, usually described as a nanoscale Lego blocks, allows to build sophisticated structures layer by layer. However, this concept has been so far limited in two dimensional (2D) materials. Here we show a class of new material where different layers are coaxially (instead of planarly) stacked. As the structure is in one dimensional (1D) form, we name it ""1D vdW heterostructures"". We demonstrate a 5 nm diameter nanotube consisting of three different materials: an inner conductive carbon nanotube (CNT), a middle insulating hexagonal boron nitride nanotube (BNNT) and an outside semiconducting MoS2 nanotube. As the technique is highly applicable to other materials in the current 2D libraries,4-6 we anticipate our strategy to be a starting point for discovering a class of new semiconducting nanotube materials. A plethora of function-designable 1D heterostructures will appear after the combination of CNTs, BNNTs and semiconducting nanotubes.        "
41,"        Second-order nonlinear optical interactions, including second harmonic generation (SHG) and sum-frequency generation (SFG), can reveal a wealth of information about chemical, electronic, and vibrational dynamics at the nanoscale. Here, we demonstrate a powerful and flexible new approach, called phase-modulated degenerate parametric amplification (DPA). The technique, which allows for facile retrieval of both the amplitude and phase of the second-order nonlinear optical response, has many advantages over conventional or heterodyne-detected SHG, including the flexibility to detect the signal at either the second harmonic or fundamental field wavelength. We demonstrate the capabilities of this approach by imaging multi-grain flakes of single-layer MoS2. We identify the absolute crystal orientation of each MoS2 domain and resolve grain boundaries with high signal contrast and sub-diffraction-limited spatial resolution. This robust all-optical method can be used to characterize structure and dynamics in organic and inorganic systems, including biological tissue, soft materials, and metal and semiconductor nanostructures, and is particularly well-suited for imaging in media that are absorptive or highly scattering to visible and ultraviolet light.        "
42,"        We report a rare atom-like interaction between excitons in monolayer WS2, measured using ultrafast absorption spectroscopy. At increasing excitation density, the exciton resonance energy exhibits a pronounced redshift followed by an anomalous blueshift. Using both material-realistic computation and phenomenological modeling, we attribute this observation to plasma effects and an attraction-repulsion crossover of the exciton-exciton interaction that mimics the Lennard-Jones potential between atoms. Our experiment demonstrates a strong analogy between excitons and atoms with respect to inter-particle interaction, which holds promise to pursue the predicted liquid and crystalline phases of excitons in two-dimensional materials.        "
43,"        Previously known to form only under high pressure synthetic conditions, here we report that the T'-type 214-structure cuprate based on the rare earth atom Tb is stabilized for ambient pressure synthesis through partial substitution of Pd for Cu. The new material is obtained in purest form for mixtures of nominal composition $Tb_{1.96}Cu_{0.80}Pd_{0.20}O_{4}$. The refined formula, in orthorhombic space group Pbca, with a = 5.5117(1) Ã, b = 5.5088(1) Ã, and c = 11.8818(1) Ã, is $Tb_{2}Cu_{0.83}Pd_{0.17}O_{4}$. An incommensurate structural modulation is seen along the a axis by electron diffraction and high resolution imaging. Magnetic susceptibility measurements reveal long range antiferromagnetic ordering at 7.9 K, with a less pronounced feature at 95 K; a magnetic moment reorientation transition is observed to onset at a field of approximately 1.1 Tesla at 3 K. The material is an n-type semiconductor.        "
44,"        The ability to confine light into tiny spatial dimensions is important for applications such as microscopy, sensing and nanoscale lasers. While plasmons offer an appealing avenue to confine light, Landau damping in metals imposes a trade-off between optical field confinement and losses. We show that a graphene-insulator-metal heterostructure can overcome that trade-off, and demonstrate plasmon confinement down to the ultimate limit of the lengthscale of one atom. This is achieved by far-field excitation of plasmon modes squeezed into an atomically thin hexagonal boron nitride dielectric h-BN spacer between graphene and metal rods. A theoretical model which takes into account the non-local optical response of both graphene and metal is used to describe the results. These ultra-confined plasmonic modes, addressed with far-field light excitation, enables a route to new regimes of ultra-strong light-matter interactions.        "
45,"        By using the Hectospec 6.5 m Multiple Mirror Telescope (MMT) and the 2.16 m telescope of National Astronomical Observatories, Chinese Academy of Sciences (NAOC), we obtained 188 high signal-to-noise ratio (S/N) spectra of HII regions in the nearby galaxy M101, which are the largest spectroscopic sample of HII regions for this galaxy so far. These spectra cover a wide range of regions on M101, which enables us to analyze two dimensional distributions of its physical properties. The physical parameters are derived from emission lines or stellar continuum, including stellar population age, electron temperature, oxygen abundance and etc. The oxygen abundances are derived using two empirical methods based on O3N2 and R$_{23}$ indicators, as well as the direct Te method when OIII$\lambda4363$ is available. By applying the harmonic decomposition analysis to the velocity field, we obtained line-of-sight rotation velocity of 71 km s$^{-1}$ and a position angle of 36 degree. The stellar age profile shows an old stellar population in galaxy center and a relative young stellar population in outer regions, suggesting an old bulge and a young disk. Oxygen abundance profile exhibits a clear break at $\sim$18 kpc, with a gradient of $-$0.0364 dex kpc$^{-1}$ in the inner region and $-$0.00686 dex kpc$^{-1}$ in the outer region. Our results agree with the ""inside-out"" disk growth scenario of M101.        "
46,"        Atomic-level structural changes in materials are important but challenging to study. Here, we demonstrate the dynamics and the possibility of manipulating a phosphorus dopant atom in graphene using scanning transmission electron microscopy (STEM). The mechanisms of various processes are explored and compared with those of other dopant species by first-principles calculations. This work paves the way for designing a more precise and optimized protocol for atomic engineering.        "
47,"        The valley pseudospin in monolayer transition metal dichalcogenides (TMDs) has been proposed as a new way to manipulate information in various optoelectronic devices. This relies on a large valley polarization that remains stable over long timescales (hundreds of ns). However, time resolved measurements report valley lifetimes of only a few ps. This has been attributed to mechanisms such as phonon-mediated inter-valley scattering and a precession of the valley psedospin through electron-hole exchange. Here we use transient spin grating to directly measure the valley depolarization lifetime in monolayer MoSe$_{2}$. We find a fast valley decay rate that scales linearly with the excitation density at different temperatures. This establishes the presence of strong exciton-exciton Coulomb exchange interactions enhancing the valley depolarization. Our work highlights the microscopic processes inhibiting the efficient use of the exciton valley pseudospin in monolayer TMDs.        "
48,"        The abundance of neutral hydrogen (HI) in satellite galaxies in the Local Group is important for studying the formation history of our Local Group. In this work, we generated mock HI satellite galaxies in the Local Group using the high mass resolution hydrodynamic \textsc{apostle} simulation. The simulated HI mass function agrees with the ALFALFA survey very well above $10^6M_{\odot}$, although there is a discrepancy below this scale because of the observed flux limit. After carefully checking various systematic elements in the observations, including fitting of line width, sky coverage, integration time, and frequency drift due to uncertainty in a galaxy's distance, we predicted the abundance of HI in galaxies in a future survey that will be conducted by FAST. FAST has a larger aperture and higher sensitivity than the Arecibo telescope. We found that the HI mass function could be estimated well around $10^5 M_{\odot}$ if the integration time is 40 minutes. Our results indicate that there are 61 HI satellites in the Local Group, and 36 in the FAST field above $10^5 M_{\odot}$. This estimation is one order of magnitude better than the current data, and will put a strong constraint on the formation history of the Local Group. Also more high resolution simulated samples are needed to achieve this target.        "
49,"        We present a spectroscopic redshift catalog from the LAMOST Complete Spectroscopic Survey of Pointing Area (LaCoSSPAr) in the Southern Galactic Cap (SGC), which is designed to observe all sources (Galactic and extra-galactic) by using repeating observations with a limiting magnitude of $r=18.1~mag$ in two $20~deg^2$ fields. The project is mainly focusing on the completeness of LAMOST ExtraGAlactic Surveys (LEGAS) in the SGC, the deficiencies of source selection methods and the basic performance parameters of LAMOST telescope. In both fields, more than 95% of galaxies have been observed. A post-processing has been applied to LAMOST 1D spectrum to remove the majority of remaining sky background residuals. More than 10,000 spectra have been visually inspected to measure the redshift by using combinations of different emission/absorption features with uncertainty of $Ï_{z}/(1+z)&lt;0.001$. In total, there are 1528 redshifts (623 absorption and 905 emission line galaxies) in Field A and 1570 redshifts (569 absorption and 1001 emission line galaxies) in Field B have been measured. The results show that it is possible to derive redshift from low SNR galaxies with our post-processing and visual inspection. Our analysis also indicates that up to 1/4 of the input targets for a typical extra-galactic spectroscopic survey might be unreliable. The multi-wavelength data analysis shows that the majority of mid-infrared-detected absorption (91.3%) and emission line galaxies (93.3%) can be well separated by an empirical criterion of $W2-W3=2.4$. Meanwhile, a fainter sequence paralleled to the main population of galaxies has been witnessed both in $M_r$/$W2-W3$ and $M_*$/$W2-W3$ diagrams, which could be the population of luminous dwarf galaxies but contaminated by the edge-on/highly inclined galaxies ($\sim30\%$).        "
50,"        This paper aims to caption daily life --i.e., to create a textual description of people's activities and interactions with objects in their homes. Addressing this problem requires novel methods beyond traditional video captioning, as most people would have privacy concerns about deploying cameras throughout their homes. We introduce RF-Diary, a new model for captioning daily life by analyzing the privacy-preserving radio signal in the home with the home's floormap. RF-Diary can further observe and caption people's life through walls and occlusions and in dark settings. In designing RF-Diary, we exploit the ability of radio signals to capture people's 3D dynamics, and use the floormap to help the model learn people's interactions with objects. We also use a multi-modal feature alignment training scheme that leverages existing video-based captioning datasets to improve the performance of our radio-based captioning model. Extensive experimental results demonstrate that RF-Diary generates accurate captions under visible conditions. It also sustains its good performance in dark or occluded settings, where video-based captioning approaches fail to generate meaningful captions. For more information, please visit our project webpage: http://rf-diary.csail.mit.edu        "
51,"        Existing domain adaptation focuses on transferring knowledge between domains with categorical indices (e.g., between datasets A and B). However, many tasks involve continuously indexed domains. For example, in medical applications, one often needs to transfer disease analysis and prediction across patients of different ages, where age acts as a continuous domain index. Such tasks are challenging for prior domain adaptation methods since they ignore the underlying relation among domains. In this paper, we propose the first method for continuously indexed domain adaptation. Our approach combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution. Our theoretical analysis demonstrates the value of leveraging the domain index to generate invariant features across a continuous range of domains. Our empirical results show that our approach outperforms the state-of-the-art domain adaption methods on both synthetic and real-world medical datasets.        "
52,"        Person Re-Identification (ReID) aims to recognize a person-of-interest across different places and times. Existing ReID methods rely on images or videos collected using RGB cameras. They extract appearance features like clothes, shoes, hair, etc. Such features, however, can change drastically from one day to the next, leading to inability to identify people over extended time periods. In this paper, we introduce RF-ReID, a novel approach that harnesses radio frequency (RF) signals for longterm person ReID. RF signals traverse clothes and reflect off the human body; thus they can be used to extract more persistent human-identifying features like body size and shape. We evaluate the performance of RF-ReID on longitudinal datasets that span days and weeks, where the person may wear different clothes across days. Our experiments demonstrate that RF-ReID outperforms state-of-the-art RGB-based ReID approaches for long term person ReID. Our results also reveal two interesting features: First since RF signals work in the presence of occlusions and poor lighting, RF-ReID allows for person ReID in such scenarios. Second, unlike photos and videos which reveal personal and private information, RF signals are more privacy-preserving, and hence can help extend person ReID to privacy-concerned domains, like healthcare.        "
53,"        Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, we propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. The learned dynamics can quickly adapt to new environments of unknown physical parameters and produce control signals to achieve a specified goal. Our experiments on manipulating ropes and controlling soft robots show that the proposed method has better efficiency and generalization ability than existing baselines.        "
54,"        Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low-rank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in Q functions. This leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to any value-based RL techniques to consistently achieve better performance on ""low-rank"" tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our approach. Code is available at https://github.com/YyzHarry/SV-RL.        "
55,"        Understanding people's actions and interactions typically depends on seeing them. Automating the process of action recognition from visual data has been the topic of much research in the computer vision community. But what if it is too dark, or if the person is occluded or behind a wall? In this paper, we introduce a neural network model that can detect human actions through walls and occlusions, and in poor lighting conditions. Our model takes radio frequency (RF) signals as input, generates 3D human skeletons as an intermediate representation, and recognizes actions and interactions of multiple people over time. By translating the input to an intermediate skeleton-based representation, our model can learn from both vision-based and RF-based datasets, and allow the two tasks to help each other. We show that our model achieves comparable accuracy to vision-based action recognition systems in visible scenarios, yet continues to work accurately when people are not visible, hence addressing scenarios that are beyond the limit of today's vision-based action recognition.        "
56,"        Deep neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examples. In contrast, the performance of defense techniques still lags behind. This paper proposes ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: first pixels are randomly dropped from the image; then, the image is reconstructed using ME. We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans typically rely on such global structures in classifying images, the process makes the network mode compatible with human perception. We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outperforms prior techniques, improving robustness against both black-box and white-box attacks.        "
57,"        We consider the problem of inferring the values of an arbitrary set of variables (e.g., risk of diseases) given other observed variables (e.g., symptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images or EEG). This is a common problem in healthcare since variables of interest often differ for different patients. Existing methods including Bayesian networks and structured prediction either do not incorporate high-dimensional signals or fail to model conditional dependencies among variables. To address these issues, we propose bidirectional inference networks (BIN), which stich together multiple probabilistic neural networks, each modeling a conditional dependency. Predictions are then made via iteratively updating variables using backpropagation (BP) to maximize corresponding posterior probability. Furthermore, we extend BIN to composite BIN (CBIN), which involves the iterative prediction process in the training stage and improves both accuracy and computational efficiency by adaptively smoothing the optimization landscape. Experiments on synthetic and real-world datasets (a sleep study and a dermatology dataset) show that CBIN is a single model that can achieve state-of-the-art performance and obtain better accuracy in most inference tasks than multiple models each specifically trained for a different task.        "
58,"        There is much interest in integrating millimeter wave radios (mmWave) into wireless LANs and 5G cellular networks to benefit from their multiple GHz of available spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios require highly directional antennas. Since the antennas have pencil-beams, the transmitter and receiver need to align their antenna beams before they can communicate. Existing solutions scan the entire space to find the best alignment. Such a process has been shown to introduce up to seconds of delay, and is unsuitable for wireless networks where an access point has to quickly switch between users and accommodate mobile clients.  This paper presents Rapid-Link, a new protocol that can find the best mmWave beam alignment without scanning the space. Given all possible directions for setting the antenna beam, Rapid-Link provably finds the optimal direction in logarithmic number of measurements. Further, Rapid-Link works within the existing 802.11ad standard for mmWave LAN, and can support both clients and access points. We have implemented Rapid-Link in a mmWave radio and evaluated it empirically. Our results show that it reduces beam alignment delay by orders of magnitude. In particular, for highly directional mmWave devices operating under 802.11ad, the delay drops from over a second to 2.5 ms.        "
59,"        Many sensor applications are interested in computing a function over measurements (e.g., sum, average, max) as opposed to collecting all sensor data. Today, such data aggregation is done in a cluster-head. Sensor nodes transmit their values sequentially to a cluster-head node, which calculates the aggregation function and forwards it to the base station. In contrast, this paper explores the possibility of computing a desired function over the air. We devise a solution that enables sensors to transmit coherently over the wireless medium so that the cluster-head directly receives the value of the desired function. We present analysis and preliminary results that demonstrate that such a design yield a large improvement in network throughput.        "
60,"        Time-of-flight, i.e., the time incurred by a signal to travel from transmitter to receiver, is perhaps the most intuitive way to measure distances using wireless signals. It is used in major positioning systems such as GPS, RADAR, and SONAR. However, attempts at using time-of-flight for indoor localization have failed to deliver acceptable accuracy due to fundamental limitations in measuring time on Wi-Fi and other RF consumer technologies. While the research community has developed alternatives for RF-based indoor localization that do not require time-of-flight, those approaches have their own limitations that hamper their use in practice. In particular, many existing approaches need receivers with large antenna arrays while commercial Wi-Fi nodes have two or three antennas. Other systems require fingerprinting the environment to create signal maps. More fundamentally, none of these methods support indoor positioning between a pair of Wi-Fi devices without~third~party~support.  In this paper, we present a set of algorithms that measure the time-of-flight to sub-nanosecond accuracy on commercial Wi-Fi cards. We implement these algorithms and demonstrate a system that achieves accurate device-to-device localization, i.e. enables a pair of Wi-Fi devices to locate each other without any support from the infrastructure, not even the location of the access points.        "
61,"        We present the first sample-optimal sublinear time algorithms for the sparse Discrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our algorithms are analyzed for /average case/ signals. For signals whose spectrum is exactly sparse, our algorithms use O(k) samples and run in O(k log k) time, where k is the expected sparsity of the signal. For signals whose spectrum is approximately sparse, our algorithm uses O(k log n) samples and runs in O(k log^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of samples used by our algorithms matches the known lower bounds for the respective signal models.  By a known reduction, our algorithms give similar results for the one-dimensional sparse Discrete Fourier Transform when n is a power of a small composite number (e.g., n = 6^t).        "
62,"        We consider the problem of computing the k-sparse approximation to the discrete Fourier transform of an n-dimensional signal. We show:  * An O(k log n)-time randomized algorithm for the case where the input signal has at most k non-zero Fourier coefficients, and  * An O(k log n log(n/k))-time randomized algorithm for general input signals.  Both algorithms achieve o(n log n) time, and thus improve over the Fast Fourier Transform, for any k = o(n). They are the first known algorithms that satisfy this property. Also, if one assumes that the Fast Fourier Transform is optimal, the algorithm for the exactly k-sparse case is optimal for any k = n^{Î©(1)}.  We complement our algorithmic results by showing that any algorithm for computing the sparse Fourier transform of a general signal must use at least Î©(k log(n/k)/ log log n) signal samples, even if it is allowed to perform adaptive sampling.        "
63,"        We examine the advantages that quantum strategies afford in communication-limited games. Inspired by the card game blackjack, we focus on cooperative, two-party sequential games in which a single classical bit of communication is allowed from the player who moves first to the player who moves second. Within this setting, we explore the usage of quantum entanglement between the players and find analytic and numerical conditions for quantum advantage over classical strategies. Using these conditions, we study a family of blackjack-type games with varying numbers of card types, and find a range of parameters where quantum advantage is achieved. Furthermore, we give an explicit quantum circuit for the strategy achieving quantum advantage.        "
64,"        Graphene has been intensively studied in photovoltaics focusing on emerging solar cells based on thin films, dye-sensitized, quantum dots, nanowires, etc. However, the typical efficiency of these solar cells incorporating graphene are below 16%. Therefore, the photovoltaic potential of graphene has not been already shown. In this work the use of graphene for concentration applications on III-V multijunction solar cells, which indeed are the solar cells with the highest efficiency, is demonstrated. Firstly, a wide optoelectronic characterization of graphene layers is carried out. Then, the graphene layer is incorporated onto triple-junction solar cells, which decreases their series resistance by 35% (relative), leading to an increase in Fill Factor of 4% (absolute) at concentrations of 1,000 suns. Simultaneously, the optical absorption of graphene produces a relative short circuit current density decrease in the range of 0-1.8%. As a result, an absolute efficiency improvement close to 1% at concentrations of 1,000 suns was achieved with respect to triple junction solar cells without graphene. The impact of incorporating one and two graphene monolayers is also evaluated.        "
65,"        Transition metal dichalcogenide (TMD) materials have emerged as promising candidates for thin film solar cells due to their wide bandgap range across the visible wavelengths, high absorption coefficient and ease of integration with both arbitrary substrates as well as conventional semiconductor technologies. However, reported TMD-based solar cells suffer from relatively low external quantum efficiencies (EQE) and low open circuit voltage due to unoptimized design and device fabrication. This paper studies $Pt/WSe_2$ vertical Schottky junction solar cells with various $WSe_2$ thicknesses in order to find the optimum absorber thickness.Also, we show that the photovoltaic performance can be improved via $Al_2O_3$ passivation which increases the EQE by up to 29.5% at 410 nm wavelength incident light. The overall resulting short circuit current improves through antireflection coating, surface doping, and surface trap passivation effects. Thanks to the ${Al_2O_3}$ coating, this work demonstrates a device with open circuit voltage ($V_{OC}$) of 380 mV and short circuit current density ($J_{SC}$) of 10.7 $mA/cm^2$. Finally, the impact of Schottky barrier height inhomogeneity at the $Pt/WSe_2$ contact is investigated as a source of open circuit voltage lowering in these devices        "
66,"        Two-dimensional (2D) transition metal nitrides (TMNs) are new members in the 2D materials family with a wide range of applications. Particularly, highly crystalline and large area thin films of TMNs are potentially promising for applications in electronic and optoelectronic devices; however, the synthesis of such TMNs has not yet been achieved. Here, we report the synthesis of few-nanometer thin Mo5N6 crystals with large area and high quality via in situ chemical conversion of layered MoS2 crystals. The structure and quality of the ultrathin Mo5N6 crystal are confirmed using transmission electron microscopy, Raman spectroscopy and X-ray photoelectron spectroscopy. The large lateral dimensions of Mo5N6 crystals are inherited from the MoS2 crystals that are used for the conversion. Atomic force microscopy characterization reveals the thickness of Mo5N6 crystals is reduced to about 1/3 of the MoS2 crystal. Electrical measurements show the obtained Mo5N6 samples are metallic with high electrical conductivity (~ 100 Î© sq-1), which is comparable to graphene. The versatility of this general approach is demonstrated by expanding the method to synthesize W5N6 and TiN. Our strategy offers a new direction for preparing 2D TMNs with desirable characteristics, opening a door for studying fundamental physics and facilitating the development of next generation electronics.        "
67,"        We unveil the diamondization mechanism of few-layer graphene compressed in the presence of water, providing robust evidence for the pressure-induced formation of 2D diamond. High-pressure Raman spectroscopy provides evidence of a phase transition occurring in the range of 4-7 GPa for 5-layer graphene and graphite. The pressure-induced phase is partially transparent and indents the silicon substrate. Our combined theoretical and experimental results indicate a gradual top-bottom diamondization mechanism, consistent with the formation of diamondene, a 2D ferromagnetic semiconductor. High-pressure x-ray diffraction on graphene indicates the formation of hexagonal diamond, consistent with the bulk limit of eclipsed-conformed diamondene.        "
68,"        Advanced microscopy and/or spectroscopy tools play indispensable role in nanoscience and nanotechnology research, as it provides rich information about the growth mechanism, chemical compositions, crystallography, and other important physical and chemical properties. However, the interpretation of imaging data heavily relies on the ""intuition"" of experienced researchers. As a result, many of the deep graphical features obtained through these tools are often unused because of difficulties in processing the data and finding the correlations. Such challenges can be well addressed by deep learning. In this work, we use the optical characterization of two-dimensional (2D) materials as a case study, and demonstrate a neural-network-based algorithm for the material and thickness identification of exfoliated 2D materials with high prediction accuracy and real-time processing capability. Further analysis shows that the trained network can extract deep graphical features such as contrast, color, edges, shapes, segment sizes and their distributions, based on which we develop an ensemble approach topredict the most relevant physical properties of 2D materials. Finally, a transfer learning technique is applied to adapt the pretrained network to other applications such as identifying layer numbers of a new 2D material, or materials produced by a different synthetic approach. Our artificial-intelligence-based material characterization approach is a powerful tool that would speed up the preparation, initial characterization of 2D materials and other nanomaterials and potentially accelerate new material discoveries.        "
69,"        When the Fermi level matches the Dirac point in graphene, the reduced charge screening can dramatically enhance electron-electron (e-e) scattering to produce a strongly interacting Dirac liquid. While the dominance of e-e scattering already leads to novel behaviors, such as electron hydrodynamic flow, further exotic phenomena have been predicted to arise specifically from the unique kinematics of e-e scattering in massless Dirac systems. Here, we use optoelectronic probes, which are highly sensitive to the kinematics of electron scattering, to uncover a giant intrinsic photocurrent response in pristine graphene. This photocurrent emerges exclusively at the charge neutrality point and vanishes abruptly at non-zero charge densities. Moreover, it is observed at places with broken reflection symmetry, and it is selectively enhanced at free graphene edges with sharp bends. Our findings reveal that the photocurrent relaxation is strongly suppressed by a drastic change of fast photocarrier kinematics in graphene when its Fermi level matches the Dirac point. The emergence of robust photocurrents in neutral Dirac materials promises new energy-harvesting functionalities and highlights intriguing electron dynamics in the optoelectronic response of Dirac fluids.        "
70,"        The effect of a two dimensional (2D) graphene layer (GL) on top of the silicon nitride (SiN) passivation layer of AlGaN/GaN metal-insulator-semiconductor high-electron-mobility transistors (MIS-HEMTs) has been systematically analyzed. Results showed that in the devices without the GL, the maximum drain current density (I_D,max) and the maximum transconductance (g_m,max) decreased gradually as the mist exposure time increased, up to 23% and 10%, respectively. Moreover, the gate lag ratio (GLR) increased around 10% during mist exposure. In contrast, devices with a GL showed a robust behavior and not significant changes in the electrical characteristics in both DC and pulsed conditions. The origin of these behaviors has been discussed and the results pointed to the GL as the key factor for improving the moisture resistance of the SiN passivation layer.        "
71,"        A Weyl semimetal (WSM) is a novel topological phase of matter, in which Weyl fermions (WFs) arise as pseudo-magnetic monopoles in its momentum space. The chirality of the WFs, given by the sign of the monopole charge, is central to the Weyl physics, since it directly serves as the sign of the topological number and gives rise to exotic properties such as Fermi arcs and the chiral anomaly. Despite being the defining property of a WSM, the chirality of the WFs has never been experimentally measured. Here, we directly detect the chirality of the WFs by measuring the photocurrent in response to circularly polarized mid-infrared light. The resulting photocurrent is determined by both the chirality of WFs and that of the photons. Our results pave the way for realizing a wide range of theoretical proposals for studying and controlling the WFs and their associated quantum anomalies by optical and electrical means. More broadly, the two chiralities, analogous to the two valleys in 2D materials, lead to a new degree of freedom in a 3D crystal with potential novel pathways to store and carry information.        "
72,"        Single photon emitters play a central role in many photonic quantum technologies. A promising class of single photon emitters consists of atomic color centers in wide-bandgap crystals, such as diamond silicon carbide and hexagonal boron nitride. However, it is currently not possible to grow these materials as sub-micron thick films on low-refractive index substrates, which is necessary for mature photonic integrated circuit technologies. Hence, there is great interest in identifying quantum emitters in technologically mature semiconductors that are compatible with suitable heteroepitaxies. Here, we demonstrate robust single photon emitters based on defects in gallium nitride (GaN), the most established and well understood semiconductor that can emit light over the entire visible spectrum. We show that the emitters have excellent photophysical properties including a brightness in excess of 500x10^3 counts/s. We further show that the emitters can be found in a variety of GaN wafers, thus offering reliable and scalable platform for further technological development. We propose a theoretical model to explain the origin of these emitters based on cubic inclusions in hexagonal gallium nitride. Our results constitute a feasible path to scalable, integrated on-chip quantum technologies based on GaN.        "
73,"        Diverse parallel stitched two-dimensional heterostructures are synthesized, including metal-semiconductor (graphene-MoS2), semiconductor-semiconductor (WS2-MoS2), and insulator-semiconductor (hBN-MoS2), directly through selective sowing of aromatic molecules as the seeds in chemical vapor deposition (CVD) method. Our methodology enables the large-scale fabrication of lateral heterostructures with arbitrary patterns, and clean and precisely aligned interfaces, which offers tremendous potential for its application in integrated circuits.        "
74,"        Graphene, owing to its ability to support plasmon polariton waves in the terahertz frequency range, enables the miniaturization of antennas to allow wireless communications among nanosystems. One of the main challenges in the demonstration of graphene antennas is finding suitable terahertz sources to feed the antenna. This paper estimates the performance of a graphene RF plasmonic micro-antenna fed with a photoconductive source. The terahertz source is modeled and, by means of a full-wave EM solver, the radiated power of the device is estimated with respect to material, laser illumination and antenna geometry parameters. The results show that the proposed device radiates terahertz pulses with an average power up to 1$Î¼$W, proving the feasibility of feeding miniaturized graphene antennas with photoconductive materials.        "
75,"        We demonstrate second order optical nonlinearity in a silicon architecture through heterogeneous integration of single-crystalline gallium nitride (GaN) on silicon (100) substrates. By engineering GaN microrings for dual resonance around 1560 nm and 780 nm, we achieve efficient, tunable second harmonic generation at 780 nm. The \{chi}(2) nonlinear susceptibility is measured to be as high as 16 plus minus 7 pm/V. Because GaN has a wideband transparency window covering ultraviolet, visible and infrared wavelengths, our platform provides a viable route for the on-chip generation of optical wavelengths in both the far infrared and near-UV through a combination of \{chi}(2) enabled sum-/difference-frequency processes.        "
76,        2D nanoelectronics based on single-layer MoS2 offers great advantages for both conventional and ubiquitous applications. This paper discusses the large-scale CVD growth of single-layer MoS2 and fabrication of devices and circuits for the first time. Both digital and analog circuits are fabricated to demonstrate its capability for mixed-signal applications.        
77,"        Two-dimensional (2D) materials, such as molybdenum disulfide (MoS2), have been shown to exhibit excellent electrical and optical properties. The semiconducting nature of MoS2 allows it to overcome the shortcomings of zero-bandgap graphene, while still sharing many of graphene's advantages for electronic and optoelectronic applications. Discrete electronic and optoelectronic components, such as field-effect transistors, sensors and photodetectors made from few-layer MoS2 show promising performance as potential substitute of Si in conventional electronics and of organic and amorphous Si semiconductors in ubiquitous systems and display applications. An important next step is the fabrication of fully integrated multi-stage circuits and logic building blocks on MoS2 to demonstrate its capability for complex digital logic and high-frequency ac applications. This paper demonstrates an inverter, a NAND gate, a static random access memory, and a five-stage ring oscillator based on a direct-coupled transistor logic technology. The circuits comprise between two to twelve transistors seamlessly integrated side-by-side on a single sheet of bilayer MoS2. Both enhancement-mode and depletion-mode transistors were fabricated thanks to the use of gate metals with different work functions.        "
78,"        In this letter, we analyze the carrier transit delay in graphene field-effect transistors (GFETs). GFETs are fabricated at the wafer-scale on sapphire substrate. For a device with a gate length of 210 nm, a current gain cut-off frequency fT of 18 GHz and 22 GHz is obtained before and after de-embedding. The extraction of the internal (Cgs,i, Cgd,i) and external capacitances (Cgs,ex and Cgd,ex) from the scaling behavior of the gate capacitances Cgs and Cgd allows the intrinsic (Ï_int), extrinsic (Ï_ext) and parasitic delays (Ï_par) to be obtained. In addition, the extraction of the intrinsic delay provides a new way to directly estimate carrier velocity from the experimental data while the breakdown of the total delay into intrinsic, extrinsic, and parasitic components can offer valuable information for optimizing RF GFETs structures.        "
79,"        In this letter, we demonstrate the first BN/Graphene/BN field effect transistor for RF applications. The BN/Graphene/BN structure can preserve the high mobility of graphene, even when it is sandwiched between a substrate and a gate dielectric. Field effect transistors (FETs) using a bilayer graphene channel have been fabricated with a gate length LG=450 nm. A current density in excess of 1 A/mm and DC transconductance close to 250 mS/mm are achieved for both electron and hole conductions. RF characterization is performed for the first time on this device structure, giving a current-gain cut-off frequency fT=33 GHz and an fT.LG product of 15 GHz.um. The improved performance obtained by the BN/Graphene/BN structure is very promising to enable the next generation of high frequency graphene RF electronics.        "
80,"        Reliable operation of photonic integrated circuits at cryogenic temperatures would enable new capabilities for emerging computing platforms, such as quantum technologies and low-power cryogenic computing. The silicon-on-insulator platform is a highly promising approach to developing large-scale photonic integrated circuits due to its exceptional manufacturability, CMOS compatibility and high component density. Fast, efficient and low-loss modulation at cryogenic temperatures in silicon, however, remains an outstanding challenge, particularly without the addition of exotic nonlinear optical materials. In this paper, we demonstrate DC-Kerr-effect-based modulation at a temperature of 5 K at GHz speeds, in a silicon photonic device fabricated exclusively within a CMOS process. This work opens up the path for the integration of DC Kerr modulators in large-scale photonic integrated circuits for emerging cryogenic classical and quantum computing applications.        "
81,"        We present a transductive learning algorithm that takes as input training examples from a distribution $P$ and arbitrary (unlabeled) test examples, possibly chosen by an adversary. This is unlike prior work that assumes that test examples are small perturbations of $P$. Our algorithm outputs a selective classifier, which abstains from predicting on some examples. By considering selective transductive learning, we give the first nontrivial guarantees for learning classes of bounded VC dimension with arbitrary train and test distributions---no prior guarantees were known even for simple classes of functions such as intervals on the line. In particular, for any function in a class $C$ of bounded VC dimension, we guarantee a low test error rate and a low rejection rate with respect to $P$. Our algorithm is efficient given an Empirical Risk Minimizer (ERM) for $C$. Our guarantees hold even for test examples chosen by an unbounded white-box adversary. We also give guarantees for generalization, agnostic, and unsupervised settings.        "
82,"        The right of an individual to request the deletion of their personal data by an entity that might be storing it -- referred to as the right to be forgotten -- has been explicitly recognized, legislated, and exercised in several jurisdictions across the world, including the European Union, Argentina, and California. However, much of the discussion surrounding this right offers only an intuitive notion of what it means for it to be fulfilled -- of what it means for such personal data to be deleted.  In this work, we provide a formal definitional framework for the right to be forgotten using tools and paradigms from cryptography. In particular, we provide a precise definition of what could be (or should be) expected from an entity that collects individuals' data when a request is made of it to delete some of this data. Our framework captures several, though not all, relevant aspects of typical systems involved in data processing. While it cannot be viewed as expressing the statements of current laws (especially since these are rather vague in this respect), our work offers technically precise definitions that represent possibilities for what the law could reasonably expect, and alternatives for what future versions of the law could explicitly require.  Finally, with the goal of demonstrating the applicability of our framework and definitions, we consider various natural and simple scenarios where the right to be forgotten comes up. For each of these scenarios, we highlight the pitfalls that arise even in genuine attempts at implementing systems offering deletion guarantees, and also describe technological solutions that provably satisfy our definitions. These solutions bring together techniques built by various communities.        "
83,"        A pseudo-deterministic algorithm is a (randomized) algorithm which, when run multiple times on the same input, with high probability outputs the same result on all executions. Classic streaming algorithms, such as those for finding heavy hitters, approximate counting, $\ell_2$ approximation, finding a nonzero entry in a vector (for turnstile algorithms) are not pseudo-deterministic. For example, in the instance of finding a nonzero entry in a vector, for any known low-space algorithm $A$, there exists a stream $x$ so that running $A$ twice on $x$ (using different randomness) would with high probability result in two different entries as the output.  In this work, we study whether it is inherent that these algorithms output different values on different executions. That is, we ask whether these problems have low-memory pseudo-deterministic algorithms. For instance, we show that there is no low-memory pseudo-deterministic algorithm for finding a nonzero entry in a vector (given in a turnstile fashion), and also that there is no low-dimensional pseudo-deterministic sketching algorithm for $\ell_2$ norm estimation. We also exhibit problems which do have low memory pseudo-deterministic algorithms but no low memory deterministic algorithm, such as outputting a nonzero row of a matrix, or outputting a basis for the row-span of a matrix.  We also investigate multi-pseudo-deterministic algorithms: algorithms which with high probability output one of a few options. We show the first lower bounds for such algorithms. This implies that there are streaming problems such that every low space algorithm for the problem must have inputs where there are many valid outputs, all with a significant probability of being outputted.        "
84,"        In [20] Goldwasser, Grossman and Holden introduced pseudo-deterministic interactive proofs for search problems where a powerful prover can convince a probabilistic polynomial time verifier that a solution to a search problem is canonical. They studied search problems for which polynomial time algorithms are not known and for which many solutions are possible. They showed that whereas there exists a constant round pseudo deterministic proof for graph isomorphism where the canonical solution is the lexicographically smallest isomorphism, the existence of pseudo-deterministic interactive proofs for NP-hard problems would imply the collapse of the polynomial time hierarchy.  In this paper, we turn our attention to studying doubly-efficient pseudo-deterministic proofs for polynomial time search problems: pseudo-deterministic proofs with the extra requirement that the prover runtime is polynomial and the verifier runtime to verify that a solution is canonical is significantly lower than the complexity of finding any solution, canonical or otherwise. Naturally this question is particularly interesting for search problems for which a lower bound on its worst case complexity is known or has been widely conjectured.  We show doubly-efficient pseudo-deterministic algorithms for a host of natural problems whose complexity has long been conjectured. In particular, we show a doubly efficient pseudo-deterministic NP proof for linear programming, 3-SUM and problems reducible to 3-SUM, the hitting set problem, and the Zero Weight Triangle problem and show a doubly-efficient pseudo-deterministic MA proof for the Orthogonal Vectors problem and the $k$-Clique problem.        "
85,"        In this paper we study the fine-grained complexity of finding exact and approximate solutions to problems in P. Our main contribution is showing reductions from exact to approximate solution for a host of such problems.  As one (notable) example, we show that the Closest-LCS-Pair problem (Given two sets of strings $A$ and $B$, compute exactly the maximum $\textsf{LCS}(a, b)$ with $(a, b) \in A \times B$) is equivalent to its approximation version (under near-linear time reductions, and with a constant approximation factor). More generally, we identify a class of problems, which we call BP-Pair-Class, comprising both exact and approximate solutions, and show that they are all equivalent under near-linear time reductions.  Exploring this class and its properties, we also show:  $\bullet$ Under the NC-SETH assumption (a significantly more relaxed assumption than SETH), solving any of the problems in this class requires essentially quadratic time.  $\bullet$ Modest improvements on the running time of known algorithms (shaving log factors) would imply that NEXP is not in non-uniform $\textsf{NC}^1$.  $\bullet$ Finally, we leverage our techniques to show new barriers for deterministic approximation algorithms for LCS.  At the heart of these new results is a deep connection between interactive proof systems for bounded-space computations and the fine-grained complexity of exact and approximate solutions to problems in P. In particular, our results build on the proof techniques from the classical IP = PSPACE result.        "
86,"        We introduce a new coordination problem in distributed computing that we call the population stability problem. A system of agents each with limited memory and communication, as well as the ability to replicate and self-destruct, is subjected to attacks by a worst-case adversary that can at a bounded rate (1) delete agents chosen arbitrarily and (2) insert additional agents with arbitrary initial state into the system. The goal is perpetually to maintain a population whose size is within a constant factor of the target size $N$. The problem is inspired by the ability of complex biological systems composed of a multitude of memory-limited individual cells to maintain a stable population size in an adverse environment. Such biological mechanisms allow organisms to heal after trauma or to recover from excessive cell proliferation caused by inflammation, disease, or normal development.  We present a population stability protocol in a communication model that is a synchronous variant of the population model of Angluin et al. In each round, pairs of agents selected at random meet and exchange messages, where at least a constant fraction of agents is matched in each round. Our protocol uses three-bit messages and $Ï(\log^2 N)$ states per agent. We emphasize that our protocol can handle an adversary that can both insert and delete agents, a setting in which existing approximate counting techniques do not seem to apply. The protocol relies on a novel coloring strategy in which the population size is encoded in the variance of the distribution of colors. Individual agents can locally obtain a weak estimate of the population size by sampling from the distribution, and make individual decisions that robustly maintain a stable global population size.        "
87,"        We introduce pseudo-deterministic interactive proofs (psdAM): interactive proof systems for search problems where the verifier is guaranteed with high probability to output the same output on different executions. As in the case with classical interactive proofs, the verifier is a probabilistic polynomial time algorithm interacting with an untrusted powerful prover.  We view pseudo-deterministic interactive proofs as an extension of the study of pseudo-deterministic randomized polynomial time algorithms: the goal of the latter is to find canonical solutions to search problems whereas the goal of the former is to prove that a solution to a search problem is canonical to a probabilistic polynomial time verifier. Alternatively, one may think of the powerful prover as aiding the probabilistic polynomial time verifier to find canonical solutions to search problems, with high probability over the randomness of the verifier. The challenge is that pseudo-determinism should hold not only with respect to the randomness, but also with respect to the prover: a malicious prover should not be able to cause the verifier to output a solution other than the unique canonical one.        "
88,"        The availability of vast amounts of data is changing how we can make medical discoveries, predict global market trends, save energy, and develop educational strategies. In some settings such as Genome Wide Association Studies or deep learning, sheer size of data seems critical. When data is held distributedly by many parties, they must share it to reap its full benefits.  One obstacle to this revolution is the lack of willingness of different parties to share data, due to reasons such as loss of privacy or competitive edge. Cryptographic works address privacy aspects, but shed no light on individual parties' losses/gains when access to data carries tangible rewards. Even if it is clear that better overall conclusions can be drawn from collaboration, are individual collaborators better off by collaborating? Addressing this question is the topic of this paper.  * We formalize a model of n-party collaboration for computing functions over private inputs in which participants receive their outputs in sequence, and the order depends on their private inputs. Each output ""improves"" on preceding outputs according to a score function.  * We say a mechanism for collaboration achieves collaborative equilibrium if it ensures higher reward for all participants when collaborating (rather than working alone). We show that in general, computing a collaborative equilibrium is NP-complete, yet we design efficient algorithms to compute it in a range of natural model settings.  Our collaboration mechanisms are in the standard model, and thus require a central trusted party; however, we show this assumption is unnecessary under standard cryptographic assumptions. We show how to implement the mechanisms in a decentralized way with new extensions of secure multiparty computation that impose order/timing constraints on output delivery to different players, as well as privacy and correctness.        "
89,"        The full-information model was introduced by Ben-Or and Linial in 1985 to study collective coin-flipping: the problem of generating a common bounded-bias bit in a network of $n$ players with $t=t(n)$ faults. They showed that the majority protocol can tolerate $t=O(\sqrt n)$ adaptive corruptions, and conjectured that this is optimal in the adaptive setting. Lichtenstein, Linial, and Saks proved that the conjecture holds for protocols in which each player sends a single bit. Their result has been the main progress on the conjecture in the last 30 years.  In this work we revisit this question and ask: what about protocols involving longer messages? Can increased communication allow for a larger fraction of faulty players?  We introduce a model of strong adaptive corruptions, where in each round, the adversary sees all messages sent by honest parties and, based on the message content, decides whether to corrupt a party (and intercept his message) or not. We prove that any one-round coin-flipping protocol, regardless of message length, is secure against at most $\tilde{O}(\sqrt n)$ strong adaptive corruptions. Thus, increased message length does not help in this setting.  We then shed light on the connection between adaptive and strongly adaptive adversaries, by proving that for any symmetric one-round coin-flipping protocol secure against $t$ adaptive corruptions, there is a symmetric one-round coin-flipping protocol secure against $t$ strongly adaptive corruptions. Returning to the standard adaptive model, we can now prove that any symmetric one-round protocol with arbitrarily long messages can tolerate at most $\tilde{O}(\sqrt n)$ adaptive corruptions.  At the heart of our results lies a novel use of the Minimax Theorem and a new technique for converting any one-round secure protocol into a protocol with messages of $polylog(n)$ bits. This technique may be of independent interest.        "
90,"        In this paper we show that the existence of general indistinguishability obfuscators conjectured in a few recent works implies, somewhat counterintuitively, strong impossibility results for virtual black box obfuscation. In particular, we show that indistinguishability obfuscation for all circuits implies:  * The impossibility of average-case virtual black box obfuscation with auxiliary input for any circuit family with super-polynomial pseudo-entropy. Such circuit families include all pseudo-random function families, and all families of encryption algorithms and randomized digital signatures that generate their required coin flips pseudo-randomly. Impossibility holds even when the auxiliary input depends only on the public circuit family, and not the specific circuit in the family being obfuscated.  * The impossibility of average-case virtual black box obfuscation with a universal simulator (with or without any auxiliary input) for any circuit family with super-polynomial pseudo-entropy.  These bounds significantly strengthen the impossibility results of Goldwasser and Kalai (STOC 2005).        "
91,"          Madhu Sudan's work spans many areas of computer science theory including computational complexity theory, the design of efficient algorithms, algorithmic coding theory, and the theory of program checking and correcting.  Two results of Sudan stand out in the impact they have had on the mathematics of computation. The first work shows a probabilistic characterization of the class NP -- those sets for which short and easily checkable proofs of membership exist, and demonstrates consequences of this characterization to classifying the complexity of approximation problems. The second work shows a polynomial time algorithm for list decoding the Reed Solomon error correcting codes.  This short note will be devoted to describing Sudan's work on probabilistically checkable proofs -- the so called {\it PCP theorem} and its implications.        "
92,"          Theoretical computer science has found fertile ground in many areas of mathematics. The approach has been to consider classical problems through the prism of computational complexity, where the number of basic computational steps taken to solve a problem is the crucial qualitative parameter. This new approach has led to a sequence of advances, in setting and solving new mathematical challenges as well as in harnessing discrete mathematics to the task of solving real-world problems.  In this talk, I will survey the development of modern cryptography -- the mathematics behind secret communications and protocols -- in this light. I will describe the complexity theoretic foundations underlying the cryptographic tasks of encryption, pseudo-randomness number generators and functions, zero knowledge interactive proofs, and multi-party secure protocols. I will attempt to highlight the paradigms and proof techniques which unify these foundations, and which have made their way into the mainstream of complexity theory.        "
